{"cells":[{"cell_type":"markdown","id":"36b63b15-fd1a-45fd-a52e-8833415d3320","metadata":{"id":"36b63b15-fd1a-45fd-a52e-8833415d3320"},"source":["# Understanding LLMs"]},{"cell_type":"code","execution_count":null,"id":"eec83704-6c0a-43ee-955a-63aa4d909841","metadata":{"id":"eec83704-6c0a-43ee-955a-63aa4d909841"},"outputs":[],"source":["from transformers import AutoTokenizer\n","import torch\n","import transformers"]},{"cell_type":"code","execution_count":null,"id":"3d799ad8-f1b9-46da-943a-0c6adc9b7f43","metadata":{"id":"3d799ad8-f1b9-46da-943a-0c6adc9b7f43"},"outputs":[],"source":["print(transformers.__version__)"]},{"cell_type":"code","execution_count":null,"id":"b7cc2c97-9fbb-4d98-85c0-537b0f30ff66","metadata":{"id":"b7cc2c97-9fbb-4d98-85c0-537b0f30ff66"},"outputs":[],"source":["print(torch.__version__)"]},{"cell_type":"markdown","id":"6486b253-05f2-4e94-950e-181a9a27c7d9","metadata":{"id":"6486b253-05f2-4e94-950e-181a9a27c7d9"},"source":["## Tokenizing Text\n","\n","### Why Tokenization?\n","\n","Tokenization transforms text into a format that models can comprehend. There are several methods for tokenizing text, each with its pros and cons:\n","\n","1. **Character-Based Tokenization**:\n","   - **Method**: Splitting the text into individual characters and assigning each a unique numerical ID.\n","   - **Pros**: Works well for languages like Chinese, where each character carries significant information.\n","   - **Cons**: Creates a small vocabulary but requires many tokens to represent a string. This can affect performance and accuracy since individual characters carry minimal information.\n","\n","2. **Word-Based Tokenization**:\n","   - **Method**: Splitting the text into individual words.\n","   - **Pros**: Captures more meaning per token.\n","   - **Cons**: Results in a large vocabulary with many unknown words (e.g., typos, slang) and different word forms (e.g., \"run\", \"runs\", \"running\").\n","\n","### Modern Tokenization Strategies\n","\n","Modern approaches balance character and word tokenization by splitting text into subwords. These methods effectively capture both the structure and meaning of the text while efficiently handling unknown words and different forms of the same word.\n","\n","- **Subword Tokenization**:\n","  - **Method**: Frequently occurring words or subwords are assigned a single token, while complex words are split into multiple tokens, each representing a meaningful part of the word.\n","  - **Example**: \"flabbergasted\" could be split into:\n","              \n","              tensor(781) \t:  fl\n","              tensor(397) \t: ab\n","              tensor(3900) \t: berg\n","              tensor(8992) \t: asted\n","\n","Different models use different tokenizers, each with its unique strategy and vocabulary size. Let's see how the GPT-2 tokenizer handles a sentence.\n","\n","### Example with GPT-2 Tokenizer\n","\n","We'll use the GPT-2 tokenizer to tokenize the sentence shown below. This involves converting the text into tokens and then decoding those tokens back into text.\n","\n","Remember: Words --> Tokens --> Unique IDs --> Vector Embeddings\\\n"]},{"cell_type":"code","execution_count":null,"id":"ed917b48-6a6c-4b98-ad37-9eeb2c902b83","metadata":{"id":"ed917b48-6a6c-4b98-ad37-9eeb2c902b83"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# Load the GPT-2 tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","input_ids = tokenizer(\"Preposterous, I'm flabbergasted!\", return_tensors=\"pt\").input_ids\n","print(input_ids)\n","# Output: tensor([[1026,  373,  257, 3223,  290, 6388,   88]])\n","\n","# Decode the tokens back into text\n","for t in input_ids[0]:\n","    print(t, \"\\t:\", tokenizer.decode(t))"]},{"cell_type":"code","execution_count":null,"id":"a273853c-f112-4bd0-b54c-e80d6e42580b","metadata":{"id":"a273853c-f112-4bd0-b54c-e80d6e42580b"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","# Load the GPT-2 tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","# Tokenize the input text\n","input_ids = tokenizer(\"I skip across the\", return_tensors=\"pt\").input_ids\n","print(input_ids)\n","# Output: tensor([[1026,  373,  257, 3223,  290, 6388,   88]])\n","\n","# Decode the tokens back into text\n","for t in input_ids[0]:\n","    print(t, \"\\t:\", tokenizer.decode(t))\n"]},{"cell_type":"markdown","id":"5840d9a8-cbc5-40b7-a31b-043ae6bbb559","metadata":{"id":"5840d9a8-cbc5-40b7-a31b-043ae6bbb559"},"source":["As shown, the tokenizer splits the input string into a series of tokens, each assigned a unique ID. Most words are represented by a single token, but longer words (or even shorter ones!) can be split into multiple tokens. Play around with this!\n","\n","### Training Tokenizers vs. Training Models\n","\n","It's important to note that training tokenizers differs from training models. Training a model is a stochastic (non-deterministic) process, while training a tokenizer is deterministic and statistical. The tokenizer learns which subwords to use based on the dataset, a design decision of the tokenization algorithm.\n","\n","Popular subword tokenization approaches include Byte-level BPE (used in GPT-2), WordPiece, and SentencePiece. Each method has its advantages and is chosen based on the specific needs of the model and dataset.\n","\n","By understanding tokenization, we can better appreciate how models process text and generate meaningful outputs."]},{"cell_type":"markdown","id":"608e9c53-a7f4-47a5-91ce-0325e972b8b3","metadata":{"id":"608e9c53-a7f4-47a5-91ce-0325e972b8b3"},"source":["## Predicting Probabilities\n","\n","\n","### Loading the Model\n","\n","First, we need to load the GPT-2 model. Here's how you do it:"]},{"cell_type":"code","execution_count":null,"id":"4bfde49d-486f-435d-a5b6-233f49626b09","metadata":{"id":"4bfde49d-486f-435d-a5b6-233f49626b09"},"outputs":[],"source":["from transformers import AutoModelForCausalLM\n","\n","gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"]},{"cell_type":"markdown","id":"48000060-6f30-4243-90e9-52416c6bebcd","metadata":{"id":"48000060-6f30-4243-90e9-52416c6bebcd"},"source":["### Understanding the Tools\n","\n","We used `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library. This library supports hundreds of models and their corresponding tokenizers. Instead of memorizing the name of each tokenizer and model class, we use `AutoTokenizer` and `AutoModelFor*`.\n","\n","For example, we use `AutoModelForCausalLM` for the causal language modeling task. The `transformers` library automatically selects the appropriate default classes based on the model's configuration. For GPT-2, this means using `GPT2Tokenizer` and `GPT2LMHeadModel` behind the scenes.\n","\n","### Feeding Input to the Model\n","\n","If we feed the tokenized sentence from the previous section into the model, we get a result back with 50,257 values for each token in the input string. Here’s how we do it:"]},{"cell_type":"code","execution_count":null,"id":"1c87d60b-8c98-43ce-96af-495f95f2668e","metadata":{"id":"1c87d60b-8c98-43ce-96af-495f95f2668e"},"outputs":[],"source":["outputs = gpt2(input_ids)\n","outputs.logits.shape"]},{"cell_type":"markdown","id":"23270fa8-0617-4592-82b4-30cfe86d5ed4","metadata":{"id":"23270fa8-0617-4592-82b4-30cfe86d5ed4"},"source":["- **First Dimension**: Number of batches (1 because we only ran a single sequence through the model).\n","- **Second Dimension**: Sequence length (number of tokens in the input sequence, 4 in our case).\n","- **Third Dimension**: Vocabulary size (~50,000).\n","\n","These are the raw model outputs, or logits, corresponding to the tokens in the vocabulary. For each input token, the model predicts the likelihood of each token in the vocabulary continuing the sequence. Higher logits mean the model considers that token a more likely continuation.\n","\n","### Converting Logits to Probabilities\n","\n","Logits are the raw outputs of the model, essentially a list of numbers like [0.1, 0.2, 0.01, ...]. We can use these logits to select the most likely token to continue the sequence. Let's see how we do that."]},{"cell_type":"markdown","id":"e1845ee8-7598-433a-91f4-de90c3eddf6e","metadata":{"id":"e1845ee8-7598-433a-91f4-de90c3eddf6e"},"source":["### Finding the Most Likely Next Token\n","\n","To focus on the logits for the entire input sentence and predict the next word, we find the index of the token with the highest value using the `argmax()` method:"]},{"cell_type":"code","execution_count":null,"id":"0fbaec0b-7c9d-4056-8e80-9759633d26eb","metadata":{"id":"0fbaec0b-7c9d-4056-8e80-9759633d26eb"},"outputs":[],"source":["# The last set of logits because we want the next word\n","# i.e. the final word after this sentence\n","final_logits = gpt2(input_ids).logits[0, -1]\n","final_logits.argmax()# gives you the token ID that has the max logit value\n","# Token ID <--> Index Location Logits"]},{"cell_type":"code","source":["#gpt2(input_ids).logits"],"metadata":{"id":"xp1E8upiG5Bm"},"id":"xp1E8upiG5Bm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4fa36745-ee7d-498a-ba90-ca1e66e37b6c","metadata":{"id":"4fa36745-ee7d-498a-ba90-ca1e66e37b6c"},"outputs":[],"source":["tokenizer.decode(final_logits.argmax())"]},{"cell_type":"markdown","id":"0c1d79f8-27f9-4790-87fd-0989075465c6","metadata":{"id":"0c1d79f8-27f9-4790-87fd-0989075465c6"},"source":["Notice how the model begins a new word with a whitespace and an \"street\". This prediction makes sense given the input sentence since it ended and its time to start another sentence. The model learns to pay attention to other tokens using an algorithm called self-attention, the fundamental building block of transformers. Self-attention allows the model to determine the significance of each token in contributing to the overall meaning of the phrase.\n","\n","### Note on Transformer Models\n","\n","Transformer models contain multiple attention layers, each specializing in different aspects of the input. Unlike heuristic systems, these features are learned during training rather than being predefined.\n","\n","By understanding how GPT-2 predicts probabilities and generates text, we can better appreciate the power and intricacy of transformer-based language models.\n","\n"]},{"cell_type":"markdown","id":"269f18e4-655a-4b74-bfa9-027b819915e5","metadata":{"id":"269f18e4-655a-4b74-bfa9-027b819915e5"},"source":["## Exploring Other Token Candidates\n","\n","Now, let's explore which other tokens were potential candidates by selecting the top 10 values. This will give us insight into the model's thought process and the alternatives it considered.\n","\n","First, we'll use PyTorch to get the top 10 logits:"]},{"cell_type":"code","execution_count":null,"id":"ff8896a4-5270-4af9-b328-d76f65f1cdb2","metadata":{"id":"ff8896a4-5270-4af9-b328-d76f65f1cdb2"},"outputs":[],"source":["import torch\n","\n","top10_logits = torch.topk(final_logits, 10)\n","for index in top10_logits.indices:\n","    print(tokenizer.decode(index))"]},{"cell_type":"markdown","id":"12215042-9dd5-4387-9bd8-e11d4a87c223","metadata":{"id":"12215042-9dd5-4387-9bd8-e11d4a87c223"},"source":["### Converting Logits to Probabilities\n","\n","Logits are raw model outputs that don't represent probabilities. To understand the model's confidence in each prediction, we need to convert these logits into probabilities. This is done by comparing each value to all other predicted values and normalizing them so that their sum equals 1. This process is called the `softmax` operation.\n","\n","Here's the code that uses `softmax` to print out the top 10 most likely tokens along with their probabilities:"]},{"cell_type":"code","execution_count":null,"id":"67049280-068b-42f9-b0f8-e57047cfb790","metadata":{"id":"67049280-068b-42f9-b0f8-e57047cfb790"},"outputs":[],"source":["top10 = torch.topk(final_logits.softmax(dim=0), 10)\n","for value, index in zip(top10.values, top10.indices):\n","    print(f\"{tokenizer.decode(index):<10} {value.item():.1%}\")"]},{"cell_type":"markdown","id":"464e83a9-4b13-46dc-8444-e5469cdaa4f3","metadata":{"id":"464e83a9-4b13-46dc-8444-e5469cdaa4f3"},"source":["### Experimenting with Predictions\n","\n","Before diving deeper, it's beneficial to experiment with the code above to understand how the model's predictions vary with different inputs. Here are some ideas to try:\n","\n","1. **Change a Few Words**: Modify the adjectives in the input string, such as \"dark\" and \"stormy\". Observe how the model's predictions change. Does it still predict \"night\"? How do the probabilities for each token shift?\n","\n","2. **Alter the Input String**: Try different input strings altogether. For instance, instead of \"It was a dark and stormy\", use \"It was a sunny and bright\". Do you agree with the model's new predictions? How do they differ?\n","\n","3. **Check Grammar**: Provide an input string that is not grammatically correct. For example, use \"It was a dark stormy and\". How does the model handle it? Look at the probabilities of the top predictions. Do the probabilities still make sense?\n","\n","By experimenting with these changes, you can gain a deeper understanding of how the model processes language and how sensitive it is to different inputs. This hands-on approach will help you appreciate the intricacies of language modeling and the strengths and limitations of transformer models like GPT-2."]},{"cell_type":"markdown","id":"48ebbead-95ac-44da-b03a-d34797fcdfe4","metadata":{"id":"48ebbead-95ac-44da-b03a-d34797fcdfe4"},"source":["## Generating Text\n","\n","Now that we understand how the model predicts the next token in a sequence, generating text becomes straightforward. By repeatedly feeding the model's predictions back into itself, we can extend the input text. The `transformers` library makes this easy with the `generate()` method, designed specifically for auto-regressive models like GPT-2. Let's explore how this works with an example.\n","\n","### Basic Text Generation\n","\n","Here’s how to use the `generate()` method to produce text:"]},{"cell_type":"code","execution_count":null,"id":"950ecbb6-ae54-4964-b848-578b6ec33d3b","metadata":{"id":"950ecbb6-ae54-4964-b848-578b6ec33d3b"},"outputs":[],"source":["\n","output_ids = gpt2.generate(input_ids, max_new_tokens=20)\n","decoded_text = tokenizer.decode(output_ids[0])\n","\n","print(\"Input IDs:\", input_ids[0])\n","print(\"Output IDs:\", output_ids)\n","print(f\"Generated text: {decoded_text}\")\n"]},{"cell_type":"markdown","id":"91db9339-6b60-4a2e-baff-86622cf340cf","metadata":{"id":"91db9339-6b60-4a2e-baff-86622cf340cf"},"source":["When we call the `generate()` method, it abstracts away the details of making multiple forward passes, predicting the next token, and appending it to the input sequence. The result is a sequence of token IDs, including both the input and the new tokens generated by the model. Using the `tokenizer.decode()` method, we can convert these token IDs back into readable text.\n","\n","### Different Strategies for Text Generation\n","\n","While the `generate()` method simplifies text generation, the strategy we use can significantly impact the quality of the generated text. The default approach, known as greedy decoding, always picks the most likely next token. This method is simple but can lead to suboptimal results, especially for longer sequences. Let's look at why and explore other strategies.\n","\n","#### Greedy Decoding\n","\n","Greedy decoding selects the most likely next token at each step:"]},{"cell_type":"code","execution_count":null,"id":"b484a7cb-b980-46c7-bae0-88e2c197ecd0","metadata":{"id":"b484a7cb-b980-46c7-bae0-88e2c197ecd0"},"outputs":[],"source":["\n","output_ids = gpt2.generate(input_ids, max_new_tokens=20)\n","decoded_text = tokenizer.decode(output_ids[0])\n","\n","print(f\"Generated text: {decoded_text}\")\n"]},{"cell_type":"markdown","id":"3323b27d-5b1c-439e-9520-5fe903a1a760","metadata":{"id":"3323b27d-5b1c-439e-9520-5fe903a1a760"},"source":["While straightforward, this method can miss more coherent sequences because it doesn't consider the overall context of the sentence. For example, given the starting phrase \"Sky,\" it might predict \"blue\" as the next word, missing out on a more contextually rich phrase like \"Sky rockets soar.\"\n","\n","#### Beam Search\n","\n","Beam search keeps track of multiple hypotheses during generation, choosing the most likely overall sequence:"]},{"cell_type":"code","execution_count":null,"id":"342324fa-6f30-4d7d-81cb-e01371dd7495","metadata":{"id":"342324fa-6f30-4d7d-81cb-e01371dd7495"},"outputs":[],"source":["\n","beam_output = gpt2.generate(\n","    input_ids,\n","    num_beams=5,\n","    max_new_tokens=30,\n",")\n","\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"79433d73-a71b-45cb-a971-cebcf45afc41","metadata":{"id":"79433d73-a71b-45cb-a971-cebcf45afc41"},"source":["Beam search is effective for tasks with predictable output lengths, like summarization or translation. However, it can be slower and sometimes still lead to repetition in open-ended generation tasks.\n","\n","#### Repetition Penalty and Bad Words (Because the model can start repeating words and/or sentences)\n","\n","To address repetition, you can introduce a repetition penalty:"]},{"cell_type":"code","execution_count":null,"id":"8ff969e3-26a7-4e09-bfd9-76d9dce6ff2e","metadata":{"id":"8ff969e3-26a7-4e09-bfd9-76d9dce6ff2e"},"outputs":[],"source":["\n","beam_output = gpt2.generate(\n","    input_ids,\n","    num_beams=5,\n","    repetition_penalty=1.9,\n","    max_new_tokens=38,\n",")\n","\n","print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"608ebe63-b753-4c0e-9f81-5a6d6b37d7b0","metadata":{"id":"608ebe63-b753-4c0e-9f81-5a6d6b37d7b0"},"source":["You can also specify `bad_words_ids` to prevent the model from generating certain tokens, such as offensive words.\n","\n","### Sampling Techniques\n","\n","Instead of always picking the most likely next token, sampling introduces randomness by sampling from the probability distribution of the next tokens.\n","\n","#### Basic Sampling"]},{"cell_type":"code","execution_count":null,"id":"1541b4ea-3e19-4077-adf8-4117d1b44d01","metadata":{"id":"1541b4ea-3e19-4077-adf8-4117d1b44d01"},"outputs":[],"source":["\n","from transformers import set_seed\n","\n","set_seed(190)\n","\n","sampling_output = gpt2.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=34,\n","    top_k=0,  # We'll revisit this parameter\n",")\n","\n","print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"23c9147e-7d9e-433e-92b9-6a61886008fd","metadata":{"id":"23c9147e-7d9e-433e-92b9-6a61886008fd"},"source":["By setting `do_sample=True`, the model picks the next token based on its probability distribution, leading to more diverse and less repetitive text.\n","\n","#### Temperature\n","\n","The `temperature` parameter adjusts the randomness of the distribution:"]},{"cell_type":"code","execution_count":null,"id":"4654eea0-d9cd-4d88-b67d-771a19c10e04","metadata":{"id":"4654eea0-d9cd-4d88-b67d-771a19c10e04"},"outputs":[],"source":["\n","sampling_output = gpt2.generate(\n","    input_ids,\n","    do_sample=True,\n","    temperature=0.0000001,\n","    max_length=40,\n","    top_k=0,\n",")\n","\n","print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"d4521593-13cd-4143-a1fe-ae450b5ecb83","metadata":{"id":"d4521593-13cd-4143-a1fe-ae450b5ecb83"},"source":["A higher temperature increases randomness, making the text more diverse but potentially less coherent. A lower temperature makes the output more predictable.\n","\n","#### Top-K Sampling\n","\n","Top-K sampling limits the selection to the top K tokens:"]},{"cell_type":"code","execution_count":null,"id":"194891af-b4c9-4b36-9a29-e7c2452cb98c","metadata":{"id":"194891af-b4c9-4b36-9a29-e7c2452cb98c"},"outputs":[],"source":["\n","sampling_output = gpt2.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=40,\n","    top_k=10,\n",")\n","\n","print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"a96efd7d-c540-47b5-8088-376ce6deadc8","metadata":{"id":"a96efd7d-c540-47b5-8088-376ce6deadc8"},"source":["This method ensures that only the most likely tokens are considered, improving quality but possibly reducing diversity.\n","\n","#### Top-P (Nucleus) Sampling\n","\n","Top-P sampling, or nucleus sampling, includes the most likely tokens whose cumulative probability exceeds a threshold:"]},{"cell_type":"code","execution_count":null,"id":"59dd5782-272f-4e79-8a7a-5ee6ea284b70","metadata":{"id":"59dd5782-272f-4e79-8a7a-5ee6ea284b70"},"outputs":[],"source":["\n","sampling_output = gpt2.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=40,\n","    top_p=0.94,\n","    top_k=0,\n",")\n","\n","print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"ed6c83d8-d4fc-4c7e-97cf-cf6a82afb4c7","metadata":{"id":"ed6c83d8-d4fc-4c7e-97cf-cf6a82afb4c7"},"source":["Top-P sampling dynamically chooses the number of tokens based on their cumulative probability, balancing quality and diversity.\n","\n","### Experimenting with Generation Strategies\n","\n","There’s no one-size-fits-all approach to text generation. Experiment with different parameters to see what works best for your specific use case. Here are some suggestions:\n","\n","1. **Parameter Tuning**: Adjust parameters like `num_beams`, `repetition_penalty`, `top_p`, and `top_k` to see how they impact the generated text.\n","2. **Avoiding Repetition**: Use `no_repeat_ngram_size` to prevent the model from repeating the same sequence of words.\n","3. **Advanced Techniques**: Explore newer methods like contrastive search, which balances probability and contextual similarity to generate coherent text while avoiding repetition."]},{"cell_type":"code","execution_count":null,"id":"91dc7756-ffcb-40be-bcfe-e1dc5f673fa9","metadata":{"id":"91dc7756-ffcb-40be-bcfe-e1dc5f673fa9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}