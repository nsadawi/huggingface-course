{"cells":[{"cell_type":"markdown","id":"eb31a3ec-ce7f-451d-801e-c334b6a838f3","metadata":{"id":"eb31a3ec-ce7f-451d-801e-c334b6a838f3"},"source":["# Understanding Diffusion"]},{"cell_type":"markdown","id":"74cd4fec-b216-4e77-bad6-81b16a0bb400","metadata":{"id":"74cd4fec-b216-4e77-bad6-81b16a0bb400"},"source":["---\n","\n","# Building Diffusion Systems with Diffusers\n","\n","The `diffusers` library is designed to be a user-friendly and flexible toolbox for building diffusion systems tailored to your use-case. At the core of this toolbox are **models** and **schedulers**. While the `DiffusionPipeline` bundles these components together for convenience, you can also unbundle the pipeline and use the models and schedulers separately to create new diffusion systems.\n","\n","# Understanding Diffusion\n","\n","---\n","\n","## Diffusion Models: A Deep Dive\n","\n","Diffusion models are an exciting advancement in the field of generative models. The high-level idea is simple yet powerful: these models take images that are blurred with noise and learn to denoise them, resulting in clear images. During training, the model sees images with varying levels of noise, and at inference time, it starts with pure noise and iteratively generates an image that looks like it came from the training data.\n","\n","### The Key Insight: Iterative Refinement\n","\n","So, what makes diffusion models so effective? Unlike previous techniques like Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), which generate images in a single pass, diffusion models create images through many steps. This iterative process allows the model to refine and correct its output gradually, leading to high-quality image generation. To see this in action, let's explore an example using the Hugging Face diffusers library.\n","\n","### Loading a Pre-Trained Diffusion Model\n","\n","We'll use the `DDPMPipeline` from the Hugging Face diffusers library to load a pre-trained diffusion model. Specifically, we'll use the `ddpm-celebahq-256` model, which was trained on the CelebA-HQ dataset—a collection of high-quality celebrity images. This model will generate images resembling those in the dataset, starting from pure noise.\n","\n","In this tutorial, you’ll learn how to use models and schedulers to assemble a diffusion system for inference. We'll start with a basic pipeline and then progress to the more complex Stable Diffusion pipeline.\n","\n","## Deconstruct a Basic Pipeline\n","\n","A pipeline is a quick and easy way to run a model for inference, requiring no more than four lines of code to generate an image:"]},{"cell_type":"code","execution_count":null,"id":"e7be2346-9232-4b82-84d5-c090f26c61b4","metadata":{"tags":[],"id":"e7be2346-9232-4b82-84d5-c090f26c61b4"},"outputs":[],"source":["\n","from diffusers import DDPMPipeline\n","\n","ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"id":"bf90aa48-4ccd-4794-8756-081ce43e0795","metadata":{"tags":[],"id":"bf90aa48-4ccd-4794-8756-081ce43e0795"},"outputs":[],"source":["# Generate a celebrity face\n","image = ddpm(num_inference_steps=30).images[0]\n","image"]},{"cell_type":"markdown","id":"bbcd03f4-0b01-410e-a808-57550571fe6c","metadata":{"id":"bbcd03f4-0b01-410e-a808-57550571fe6c"},"source":["This generates an image of a fake celebrity using the `DDPMPipeline`. But how does it work under the hood? Let’s break down the pipeline and see what’s happening.\n","\n","### Understanding the Pipeline\n","\n","In the example above, the pipeline contains a `UNet2DModel` model and a `DDPMScheduler`. The pipeline denoises an image by taking random noise the size of the desired output and passing it through the model several times. At each timestep, the model predicts the noise residual and the scheduler uses it to predict a less noisy image. This process repeats until it reaches the end of the specified number of inference steps.\n","\n","To recreate the pipeline with the model and scheduler separately, let’s write our own denoising process.\n","\n","### Load the Model and Scheduler\n","\n","First, we need to load the model and scheduler:"]},{"cell_type":"code","execution_count":null,"id":"55ec96aa-adbe-4e23-9888-a251973c546b","metadata":{"tags":[],"id":"55ec96aa-adbe-4e23-9888-a251973c546b"},"outputs":[],"source":["\n","from diffusers import DDPMScheduler, UNet2DModel\n","\n","scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n","model = UNet2DModel.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")\n"]},{"cell_type":"markdown","id":"ea2b8534-08a3-422a-b8d1-4aa8fbf200e6","metadata":{"id":"ea2b8534-08a3-422a-b8d1-4aa8fbf200e6"},"source":["### Set the Number of Timesteps\n","\n","Next, we set the number of timesteps to run the denoising process for:"]},{"cell_type":"code","execution_count":null,"id":"d13b1361-0615-45e1-9923-d913a5c1f48f","metadata":{"tags":[],"id":"d13b1361-0615-45e1-9923-d913a5c1f48f"},"outputs":[],"source":["\n","scheduler.set_timesteps(50)\n"]},{"cell_type":"markdown","id":"282a0956-a060-4be7-9f36-3488b27ea580","metadata":{"id":"282a0956-a060-4be7-9f36-3488b27ea580"},"source":["Setting the scheduler timesteps creates a tensor with evenly spaced elements, 50 in this example. Each element corresponds to a timestep at which the model denoises an image:"]},{"cell_type":"code","execution_count":null,"id":"bc37c599-2d30-4533-b84b-a228058d41ca","metadata":{"tags":[],"id":"bc37c599-2d30-4533-b84b-a228058d41ca"},"outputs":[],"source":["\n","scheduler.timesteps\n"]},{"cell_type":"markdown","id":"28a0efe2-9fa7-4831-8fbe-9cf62b008da9","metadata":{"id":"28a0efe2-9fa7-4831-8fbe-9cf62b008da9"},"source":["### Create Random Noise\n","\n","Create some random noise with the same shape as the desired output:"]},{"cell_type":"code","execution_count":null,"id":"606096a0-f8aa-4718-b4d2-b5b77ba9543c","metadata":{"tags":[],"id":"606096a0-f8aa-4718-b4d2-b5b77ba9543c"},"outputs":[],"source":["import torch\n","\n","# Get the sample size from the model configuration\n","sample_size = model.config.sample_size\n","\n","# Create random noise with the same shape as the desired output\n","noise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")\n"]},{"cell_type":"markdown","id":"3e1be17e-cb13-4563-937a-abcd60d56b9f","metadata":{"id":"3e1be17e-cb13-4563-937a-abcd60d56b9f"},"source":["### Write the Denoising Loop\n","\n","Now we write a loop to iterate over the timesteps. At each timestep, the model does a `UNet2DModel.forward()` pass and returns the noisy residual. The scheduler’s `step()` method takes the noisy residual, timestep, and input, and predicts the image at the previous timestep. This output becomes the next input to the model in the denoising loop, repeating until the end of the timesteps array.\n","\n","\n","In each iteration:\n","\n","* The model predicts the noise in the current image (noisy_residual).\n","* The scheduler's step method uses this prediction to estimate the image at the previous timestep (previous_noisy_sample).\n","* This new estimate becomes the input for the next iteration.\n","* This process continues until we have stepped through all timesteps, progressively denoising the image."]},{"cell_type":"code","execution_count":null,"id":"3524c887-f028-42b7-ba67-0591516b006f","metadata":{"tags":[],"id":"3524c887-f028-42b7-ba67-0591516b006f"},"outputs":[],"source":["# Initialize the input to be the random noise we created\n","input = noise\n","\n","# Loop over each timestep\n","for t in scheduler.timesteps:\n","    with torch.no_grad():  # No gradient calculation is needed\n","        # Get the noisy residual from the model\n","        noisy_residual = model(input, t).sample\n","    # Predict the image at the previous timestep\n","    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n","    # Update the input for the next iteration\n","    input = previous_noisy_sample\n"]},{"cell_type":"markdown","id":"9f2d421e-69db-4e80-9b1f-df6360768ac3","metadata":{"id":"9f2d421e-69db-4e80-9b1f-df6360768ac3"},"source":["This is the entire denoising process. You can use this same pattern to write any diffusion system.\n","\n","### Convert the Denoised Output to an Image\n","\n","The last step is to convert the denoised output into an image.\n","Here’s what happens in this step:\n","\n","* We normalize the tensor values to the range [0, 1] and squeeze any singleton dimensions.\n","* We permute the dimensions to match the format expected by PIL (Height x Width x Channels).\n","* The values are scaled to [0, 255] and converted to an 8-bit unsigned integer format.\n","* Finally, we convert the NumPy array to a PIL image and display it."]},{"cell_type":"code","execution_count":null,"id":"7b4c0636-c182-4114-9f08-0e62a72c92c9","metadata":{"tags":[],"id":"7b4c0636-c182-4114-9f08-0e62a72c92c9"},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","\n","# Normalize the image data to be between 0 and 1\n","image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n","# Change the shape and type for image conversion\n","# The Python imaging library expects (w,h,ch)\n","image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n","# Create a PIL image\n","image = Image.fromarray(image)\n","image\n"]},{"cell_type":"markdown","source":["# Another Example with Stability AI Stable Diffusion Model"],"metadata":{"id":"ZTPwhe5i4LVL"},"id":"ZTPwhe5i4LVL"},{"cell_type":"code","execution_count":null,"id":"9961143e-2093-442a-a1d9-42140dbf0d09","metadata":{"tags":[],"id":"9961143e-2093-442a-a1d9-42140dbf0d09"},"outputs":[],"source":["from PIL import Image\n","import torch\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n","\n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n","tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n","text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True)\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True)\n"]},{"cell_type":"markdown","id":"550e39f1-5298-45a6-b6b8-efa19fb26aa8","metadata":{"id":"550e39f1-5298-45a6-b6b8-efa19fb26aa8"},"source":["### Use a Different Scheduler\n","\n","Instead of the default `PNDMScheduler`, let's use the `UniPCMultistepScheduler`:"]},{"cell_type":"code","execution_count":null,"id":"90c053c6-b0a2-443b-af3d-845cdc60bcd5","metadata":{"tags":[],"id":"90c053c6-b0a2-443b-af3d-845cdc60bcd5"},"outputs":[],"source":["\n","from diffusers import UniPCMultistepScheduler\n","\n","scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n"]},{"cell_type":"markdown","id":"0cdb7172-b46f-4f31-9f1c-47ef821418fd","metadata":{"id":"0cdb7172-b46f-4f31-9f1c-47ef821418fd"},"source":["### Move Models to GPU\n","\n","To speed up inference, move the models to a GPU:"]},{"cell_type":"code","execution_count":null,"id":"64d56d75-bca4-4f42-84ad-50c8ea3e1845","metadata":{"tags":[],"id":"64d56d75-bca4-4f42-84ad-50c8ea3e1845"},"outputs":[],"source":["\n","torch_device = \"cuda\"\n","vae.to(torch_device)\n","text_encoder.to(torch_device)\n","unet.to(torch_device)\n"]},{"cell_type":"markdown","id":"447c1e1b-045a-4f18-970d-66d38b7c1e9e","metadata":{"id":"447c1e1b-045a-4f18-970d-66d38b7c1e9e"},"source":["### Create Text Embeddings\n","\n","Tokenize the text to generate embeddings. The text is used to condition the UNet model and steer the diffusion process towards something that resembles the input prompt:"]},{"cell_type":"code","execution_count":null,"id":"f1d83b3a-cc0d-4cf8-8154-d8ff33ce2ca4","metadata":{"tags":[],"id":"f1d83b3a-cc0d-4cf8-8154-d8ff33ce2ca4"},"outputs":[],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"id":"107d7be8-d250-4da4-8bf0-383d4a0ae653","metadata":{"tags":[],"id":"107d7be8-d250-4da4-8bf0-383d4a0ae653"},"outputs":[],"source":["prompt = [\"a nice flower\"]\n","height = 512  # default height of Stable Diffusion\n","width = 512  # default width of Stable Diffusion\n","num_inference_steps = 25  # Number of denoising steps\n","guidance_scale = 7.5  # Scale for classifier-free guidance"]},{"cell_type":"code","execution_count":null,"id":"ec85a63b-223e-41d7-b8f6-4497c847d389","metadata":{"tags":[],"id":"ec85a63b-223e-41d7-b8f6-4497c847d389"},"outputs":[],"source":["generator = torch.manual_seed(0)  # Seed generator to create the initial latent noise"]},{"cell_type":"code","execution_count":null,"id":"9aa8cc4d-1dd7-4123-9c83-629f7835a713","metadata":{"tags":[],"id":"9aa8cc4d-1dd7-4123-9c83-629f7835a713"},"outputs":[],"source":["batch_size = len(prompt)\n","\n","text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n","\n","max_length = text_input.input_ids.shape[-1]\n","uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n","uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n","\n","text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n"]},{"cell_type":"markdown","id":"1bf76db1-321a-4f50-8418-3c55ae41f8e2","metadata":{"id":"1bf76db1-321a-4f50-8418-3c55ae41f8e2"},"source":["### Create Random Noise\n","\n","Generate some initial random noise as a starting point for the diffusion process:"]},{"cell_type":"code","execution_count":null,"id":"01fdd4cd-01c4-4aca-95bb-c858bb2afb25","metadata":{"tags":[],"id":"01fdd4cd-01c4-4aca-95bb-c858bb2afb25"},"outputs":[],"source":["\n","latents = torch.randn((batch_size, unet.config.in_channels, height // 8, width // 8), device=torch_device)\n","latents = latents * scheduler.init_noise_sigma\n"]},{"cell_type":"markdown","id":"46721722-8095-4566-bdfe-1f9fcbbf28e7","metadata":{"id":"46721722-8095-4566-bdfe-1f9fcbbf28e7"},"source":["### Denoise the Image\n","\n","Create the denoising loop to progressively transform the pure noise in latents to an image described by your prompt:"]},{"cell_type":"code","execution_count":null,"id":"e1d4606f-4b67-4601-b427-c5e65ba2de36","metadata":{"tags":[],"id":"e1d4606f-4b67-4601-b427-c5e65ba2de36"},"outputs":[],"source":["\n","from tqdm.auto import tqdm\n","\n","scheduler.set_timesteps(num_inference_steps)\n","\n","for t in tqdm(scheduler.timesteps):\n","    latent_model_input = torch.cat([latents] * 2)\n","    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n","\n","    with torch.no_grad():\n","        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","\n","    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","\n","    latents = scheduler.step(noise_pred, t, latents).prev_sample\n"]},{"cell_type":"markdown","id":"59413e4e-a0a8-43e0-9ebf-85ede64a889b","metadata":{"id":"59413e4e-a0a8-43e0-9ebf-85ede64a889b"},"source":["### Decode the Image\n","\n","Finally, use the VAE to decode the latent representation into an image:"]},{"cell_type":"code","execution_count":null,"id":"0fc8e0d8-dddb-4b4a-b9cb-48028783ea8a","metadata":{"tags":[],"id":"0fc8e0d8-dddb-4b4a-b9cb-48028783ea8a"},"outputs":[],"source":["\n","latents = 1/ 0.18215 * latents\n","with torch.no_grad():\n","    image = vae.decode(latents).sample\n","\n","image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n","image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n","image = Image.fromarray(image)\n","image\n"]},{"cell_type":"markdown","id":"56e15bf2-f64f-4d51-b301-0e2e6335772b","metadata":{"id":"56e15bf2-f64f-4d51-b301-0e2e6335772b"},"source":["And that's it! You've now created and understood a diffusion system using the `diffusers` library, both for a basic and a Stable Diffusion pipeline. Feel free to experiment with different models and settings to see what other amazing images you can generate!\n"]},{"cell_type":"code","execution_count":null,"id":"02f521a6-ec8a-44cb-9551-1c344b3d7827","metadata":{"id":"02f521a6-ec8a-44cb-9551-1c344b3d7827"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","\n","pipe = DiffusionPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\")\n","pipe.load_lora_weights(\"strangerzonehf/Flux-Midjourney-Mix2-LoRA\")\n","\n","prompt = \"street photography, dark green background --ar 47:64 --v 6.0 --style raw\"\n","image = pipe(prompt).images[0]"]},{"cell_type":"code","source":[],"metadata":{"id":"OBonfF9CElHz"},"id":"OBonfF9CElHz","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}